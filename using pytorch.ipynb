{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train= pd.read_csv('train.csv')\n",
    "test= pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train.drop(['label'],axis=1)\n",
    "Y=train['label']\n",
    "X=torch.from_numpy(np.array(X).reshape(-1,1,28,28)/255)\n",
    "Y=torch.from_numpy(np.array(Y))\n",
    "xtrain,xval,ytrain,yval=train_test_split(X,Y,test_size=0.2,random_state=2019)\n",
    "\n",
    "train_batch_size=512\n",
    "val_batch_size=512\n",
    "train_dataset=torch.utils.data.TensorDataset(xtrain.type(torch.FloatTensor),ytrain.type(torch.LongTensor))\n",
    "train_loader=torch.utils.data.DataLoader(train_dataset,batch_size=train_batch_size,shuffle=True)\n",
    "val_dataset=torch.utils.data.TensorDataset(xval.type(torch.FloatTensor),yval.type(torch.LongTensor))\n",
    "val_loader=torch.utils.data.DataLoader(val_dataset,batch_size=val_batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self,input):\n",
    "        return input.view(-1,64*3*3)\n",
    "model=nn.Sequential(\n",
    "    nn.Conv2d(1,16,3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(16,32,3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Conv2d(32,64,3,padding=1),\n",
    "    nn.ReLU(),\n",
    "    nn.MaxPool2d(2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(64*3*3,120),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(120,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,10)                    \n",
    ")\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model,dataload,criterion):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    los=0\n",
    "    for images,labels in dataload:\n",
    "        #images=images.view(images.shape[0],784)\n",
    "        images,labels=images.to(device),labels.to(device)\n",
    "        output=model(images)\n",
    "\n",
    "        test_loss=criterion(output,labels)\n",
    "        los+=test_loss.item()\n",
    "        preds = torch.argmax(output,1)\n",
    "        running_loss += test_loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "    #print(output.shape)\n",
    "    #epoch_loss = running_loss / len(xval)\n",
    "    #print(type(running_loss))\n",
    "    #print(epoch_loss),print(los/len(dataload))        \n",
    "    epoch_loss = running_loss / len(xval)\n",
    "    epoch_acc = running_corrects.double() / len(xval)\n",
    "    \n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\"valid\", epoch_loss, epoch_acc))\n",
    "    return epoch_acc,epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/210:\n",
      "----------\n",
      "Training Loss: 1.2609 Acc: 0.6014\n",
      "valid Loss: 0.4575 Acc: 0.8392\n",
      "Epoch 2/210:\n",
      "----------\n",
      "Training Loss: 0.2798 Acc: 0.9135\n",
      "valid Loss: 0.1938 Acc: 0.9412\n",
      "Epoch 3/210:\n",
      "----------\n",
      "Training Loss: 0.1718 Acc: 0.9471\n",
      "valid Loss: 0.1425 Acc: 0.9557\n",
      "Epoch 4/210:\n",
      "----------\n",
      "Training Loss: 0.1262 Acc: 0.9608\n",
      "valid Loss: 0.1252 Acc: 0.9606\n",
      "Epoch 5/210:\n",
      "----------\n",
      "Training Loss: 0.1029 Acc: 0.9677\n",
      "valid Loss: 0.0945 Acc: 0.9679\n",
      "Epoch 6/210:\n",
      "----------\n",
      "Training Loss: 0.0879 Acc: 0.9735\n",
      "valid Loss: 0.0952 Acc: 0.9694\n",
      "Epoch 7/210:\n",
      "----------\n",
      "Training Loss: 0.0751 Acc: 0.9771\n",
      "valid Loss: 0.0740 Acc: 0.9746\n",
      "Epoch 8/210:\n",
      "----------\n",
      "Training Loss: 0.0685 Acc: 0.9785\n",
      "valid Loss: 0.0718 Acc: 0.9771\n",
      "Epoch 9/210:\n",
      "----------\n",
      "Training Loss: 0.0590 Acc: 0.9818\n",
      "valid Loss: 0.0623 Acc: 0.9805\n",
      "Epoch 10/210:\n",
      "----------\n",
      "Training Loss: 0.0545 Acc: 0.9833\n",
      "valid Loss: 0.0764 Acc: 0.9740\n",
      "best acc is 0.9805\n",
      "Epoch 11/210:\n",
      "----------\n",
      "Training Loss: 0.0505 Acc: 0.9841\n",
      "valid Loss: 0.0608 Acc: 0.9796\n",
      "best acc is 0.9805\n",
      "Epoch 12/210:\n",
      "----------\n",
      "Training Loss: 0.0457 Acc: 0.9859\n",
      "valid Loss: 0.0558 Acc: 0.9818\n",
      "Epoch 13/210:\n",
      "----------\n",
      "Training Loss: 0.0409 Acc: 0.9872\n",
      "valid Loss: 0.0578 Acc: 0.9813\n",
      "best acc is 0.9818\n",
      "Epoch 14/210:\n",
      "----------\n",
      "Training Loss: 0.0441 Acc: 0.9857\n",
      "valid Loss: 0.0458 Acc: 0.9857\n",
      "Epoch 15/210:\n",
      "----------\n",
      "Training Loss: 0.0333 Acc: 0.9900\n",
      "valid Loss: 0.0486 Acc: 0.9833\n",
      "best acc is 0.9857\n",
      "Epoch 16/210:\n",
      "----------\n",
      "Training Loss: 0.0316 Acc: 0.9905\n",
      "valid Loss: 0.0733 Acc: 0.9767\n",
      "best acc is 0.9857\n",
      "Epoch 17/210:\n",
      "----------\n",
      "Training Loss: 0.0315 Acc: 0.9900\n",
      "valid Loss: 0.0487 Acc: 0.9835\n",
      "best acc is 0.9857\n",
      "Epoch 18/210:\n",
      "----------\n",
      "Training Loss: 0.0302 Acc: 0.9905\n",
      "valid Loss: 0.0523 Acc: 0.9840\n",
      "best acc is 0.9857\n",
      "Epoch 19/210:\n",
      "----------\n",
      "Training Loss: 0.0253 Acc: 0.9917\n",
      "valid Loss: 0.0392 Acc: 0.9877\n",
      "Epoch 20/210:\n",
      "----------\n",
      "Training Loss: 0.0237 Acc: 0.9926\n",
      "valid Loss: 0.0506 Acc: 0.9839\n",
      "best acc is 0.9877\n",
      "Epoch 21/210:\n",
      "----------\n",
      "Training Loss: 0.0205 Acc: 0.9933\n",
      "valid Loss: 0.0462 Acc: 0.9857\n",
      "best acc is 0.9877\n",
      "Epoch 22/210:\n",
      "----------\n",
      "Training Loss: 0.0208 Acc: 0.9935\n",
      "valid Loss: 0.0569 Acc: 0.9825\n",
      "best acc is 0.9877\n",
      "Epoch 23/210:\n",
      "----------\n",
      "Training Loss: 0.0220 Acc: 0.9925\n",
      "valid Loss: 0.0635 Acc: 0.9811\n",
      "best acc is 0.9877\n",
      "Epoch 24/210:\n",
      "----------\n",
      "Training Loss: 0.0250 Acc: 0.9914\n",
      "valid Loss: 0.0415 Acc: 0.9867\n",
      "best acc is 0.9877\n",
      "Epoch 25/210:\n",
      "----------\n",
      "Training Loss: 0.0170 Acc: 0.9948\n",
      "valid Loss: 0.0406 Acc: 0.9871\n",
      "best acc is 0.9877\n",
      "Epoch 26/210:\n",
      "----------\n",
      "Training Loss: 0.0161 Acc: 0.9949\n",
      "valid Loss: 0.0363 Acc: 0.9881\n",
      "Epoch 27/210:\n",
      "----------\n",
      "Training Loss: 0.0168 Acc: 0.9949\n",
      "valid Loss: 0.0368 Acc: 0.9868\n",
      "best acc is 0.9881\n",
      "Epoch 28/210:\n",
      "----------\n",
      "Training Loss: 0.0126 Acc: 0.9966\n",
      "valid Loss: 0.0403 Acc: 0.9874\n",
      "best acc is 0.9881\n",
      "Epoch 29/210:\n",
      "----------\n",
      "Training Loss: 0.0140 Acc: 0.9955\n",
      "valid Loss: 0.0408 Acc: 0.9874\n",
      "best acc is 0.9881\n",
      "Epoch 30/210:\n",
      "----------\n",
      "Training Loss: 0.0111 Acc: 0.9965\n",
      "valid Loss: 0.0420 Acc: 0.9867\n",
      "best acc is 0.9881\n",
      "Epoch 31/210:\n",
      "----------\n",
      "Training Loss: 0.0113 Acc: 0.9967\n",
      "valid Loss: 0.0508 Acc: 0.9858\n",
      "best acc is 0.9881\n",
      "Epoch 32/210:\n",
      "----------\n",
      "Training Loss: 0.0110 Acc: 0.9968\n",
      "valid Loss: 0.0399 Acc: 0.9888\n",
      "Epoch 33/210:\n",
      "----------\n",
      "Training Loss: 0.0093 Acc: 0.9972\n",
      "valid Loss: 0.0427 Acc: 0.9871\n",
      "best acc is 0.9888\n",
      "Epoch 34/210:\n",
      "----------\n",
      "Training Loss: 0.0075 Acc: 0.9980\n",
      "valid Loss: 0.0436 Acc: 0.9867\n",
      "best acc is 0.9888\n",
      "Epoch 35/210:\n",
      "----------\n",
      "Training Loss: 0.0082 Acc: 0.9976\n",
      "valid Loss: 0.0460 Acc: 0.9869\n",
      "best acc is 0.9888\n",
      "Epoch 36/210:\n",
      "----------\n",
      "Training Loss: 0.0074 Acc: 0.9977\n",
      "valid Loss: 0.0406 Acc: 0.9881\n",
      "best acc is 0.9888\n",
      "Epoch 37/210:\n",
      "----------\n",
      "Training Loss: 0.0092 Acc: 0.9968\n",
      "valid Loss: 0.0516 Acc: 0.9860\n",
      "best acc is 0.9888\n",
      "Epoch 38/210:\n",
      "----------\n",
      "Training Loss: 0.0088 Acc: 0.9970\n",
      "valid Loss: 0.0593 Acc: 0.9844\n",
      "best acc is 0.9888\n",
      "Epoch 39/210:\n",
      "----------\n",
      "Training Loss: 0.0065 Acc: 0.9982\n",
      "valid Loss: 0.0453 Acc: 0.9876\n",
      "best acc is 0.9888\n",
      "Epoch 40/210:\n",
      "----------\n",
      "Training Loss: 0.0058 Acc: 0.9983\n",
      "valid Loss: 0.0459 Acc: 0.9873\n",
      "best acc is 0.9888\n",
      "Epoch 41/210:\n",
      "----------\n",
      "Training Loss: 0.0067 Acc: 0.9980\n",
      "valid Loss: 0.0513 Acc: 0.9861\n",
      "best acc is 0.9888\n",
      "Epoch 42/210:\n",
      "----------\n",
      "Training Loss: 0.0058 Acc: 0.9982\n",
      "valid Loss: 0.0474 Acc: 0.9880\n",
      "best acc is 0.9888\n",
      "Epoch 43/210:\n",
      "----------\n",
      "Training Loss: 0.0048 Acc: 0.9984\n",
      "valid Loss: 0.0521 Acc: 0.9856\n",
      "best acc is 0.9888\n",
      "Epoch 44/210:\n",
      "----------\n",
      "Training Loss: 0.0032 Acc: 0.9992\n",
      "valid Loss: 0.0454 Acc: 0.9881\n",
      "best acc is 0.9888\n",
      "Epoch 45/210:\n",
      "----------\n",
      "Training Loss: 0.0029 Acc: 0.9993\n",
      "valid Loss: 0.0480 Acc: 0.9879\n",
      "best acc is 0.9888\n",
      "Epoch 46/210:\n",
      "----------\n",
      "Training Loss: 0.0034 Acc: 0.9990\n",
      "valid Loss: 0.0528 Acc: 0.9875\n",
      "best acc is 0.9888\n",
      "Epoch 47/210:\n",
      "----------\n",
      "Training Loss: 0.0034 Acc: 0.9990\n",
      "valid Loss: 0.0543 Acc: 0.9864\n",
      "best acc is 0.9888\n",
      "Epoch 48/210:\n",
      "----------\n",
      "Training Loss: 0.0079 Acc: 0.9973\n",
      "valid Loss: 0.0486 Acc: 0.9879\n",
      "best acc is 0.9888\n",
      "Epoch 49/210:\n",
      "----------\n",
      "Training Loss: 0.0028 Acc: 0.9994\n",
      "valid Loss: 0.0435 Acc: 0.9892\n",
      "Epoch 50/210:\n",
      "----------\n",
      "Training Loss: 0.0045 Acc: 0.9985\n",
      "valid Loss: 0.0596 Acc: 0.9867\n",
      "best acc is 0.9892\n",
      "Epoch 51/210:\n",
      "----------\n",
      "Training Loss: 0.0023 Acc: 0.9994\n",
      "valid Loss: 0.0503 Acc: 0.9882\n",
      "best acc is 0.9892\n",
      "Epoch 52/210:\n",
      "----------\n",
      "Training Loss: 0.0044 Acc: 0.9985\n",
      "valid Loss: 0.0625 Acc: 0.9854\n",
      "best acc is 0.9892\n",
      "Epoch 53/210:\n",
      "----------\n",
      "Training Loss: 0.0062 Acc: 0.9978\n",
      "valid Loss: 0.0622 Acc: 0.9868\n",
      "best acc is 0.9892\n",
      "Epoch 54/210:\n",
      "----------\n",
      "Training Loss: 0.0120 Acc: 0.9958\n",
      "valid Loss: 0.0537 Acc: 0.9865\n",
      "best acc is 0.9892\n",
      "Epoch 55/210:\n",
      "----------\n",
      "Training Loss: 0.0052 Acc: 0.9982\n",
      "valid Loss: 0.0624 Acc: 0.9854\n",
      "best acc is 0.9892\n",
      "Epoch 56/210:\n",
      "----------\n",
      "Training Loss: 0.0027 Acc: 0.9993\n",
      "valid Loss: 0.0558 Acc: 0.9875\n",
      "best acc is 0.9892\n",
      "Epoch 57/210:\n",
      "----------\n",
      "Training Loss: 0.0012 Acc: 0.9997\n",
      "valid Loss: 0.0535 Acc: 0.9879\n",
      "best acc is 0.9892\n",
      "Epoch 58/210:\n",
      "----------\n",
      "Training Loss: 0.0024 Acc: 0.9992\n",
      "valid Loss: 0.0598 Acc: 0.9876\n",
      "best acc is 0.9892\n",
      "Epoch 59/210:\n",
      "----------\n",
      "Training Loss: 0.0010 Acc: 0.9999\n",
      "valid Loss: 0.0487 Acc: 0.9888\n",
      "best acc is 0.9892\n",
      "Epoch 60/210:\n",
      "----------\n",
      "Training Loss: 0.0005 Acc: 0.9999\n",
      "valid Loss: 0.0495 Acc: 0.9889\n",
      "best acc is 0.9892\n",
      "Epoch 61/210:\n",
      "----------\n",
      "Training Loss: 0.0006 Acc: 1.0000\n",
      "valid Loss: 0.0477 Acc: 0.9902\n",
      "Epoch 62/210:\n",
      "----------\n",
      "Training Loss: 0.0002 Acc: 1.0000\n",
      "valid Loss: 0.0493 Acc: 0.9894\n",
      "best acc is 0.9902\n",
      "Epoch 63/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0507 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 64/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0507 Acc: 0.9895\n",
      "best acc is 0.9902\n",
      "Epoch 65/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0508 Acc: 0.9895\n",
      "best acc is 0.9902\n",
      "Epoch 66/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0524 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 67/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0516 Acc: 0.9894\n",
      "best acc is 0.9902\n",
      "Epoch 68/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0534 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 69/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0545 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 70/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0534 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 71/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0546 Acc: 0.9893\n",
      "best acc is 0.9902\n",
      "Epoch 72/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0543 Acc: 0.9894\n",
      "best acc is 0.9902\n",
      "Epoch 73/210:\n",
      "----------\n",
      "Training Loss: 0.0001 Acc: 1.0000\n",
      "valid Loss: 0.0544 Acc: 0.9896\n",
      "best acc is 0.9902\n",
      "Epoch 74/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0548 Acc: 0.9894\n",
      "best acc is 0.9902\n",
      "Epoch 75/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0563 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 76/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0554 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 77/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0560 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 78/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0560 Acc: 0.9894\n",
      "best acc is 0.9902\n",
      "Epoch 79/210:\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0564 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 80/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0563 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 81/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0579 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 82/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0573 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 83/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0580 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 84/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0586 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 85/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0588 Acc: 0.9894\n",
      "best acc is 0.9902\n",
      "Epoch 86/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0590 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 87/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0592 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 88/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0587 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 89/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0590 Acc: 0.9893\n",
      "best acc is 0.9902\n",
      "Epoch 90/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0601 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 91/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0599 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 92/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0597 Acc: 0.9894\n",
      "best acc is 0.9902\n",
      "Epoch 93/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0602 Acc: 0.9893\n",
      "best acc is 0.9902\n",
      "Epoch 94/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0615 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 95/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0613 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 96/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0607 Acc: 0.9894\n",
      "best acc is 0.9902\n",
      "Epoch 97/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0613 Acc: 0.9893\n",
      "best acc is 0.9902\n",
      "Epoch 98/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0616 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 99/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0623 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 100/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0622 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 101/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0627 Acc: 0.9893\n",
      "best acc is 0.9902\n",
      "Epoch 102/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0622 Acc: 0.9893\n",
      "best acc is 0.9902\n",
      "Epoch 103/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0628 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 104/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0637 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 105/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0640 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 106/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0632 Acc: 0.9894\n",
      "best acc is 0.9902\n",
      "Epoch 107/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0642 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 108/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0651 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 109/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0648 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 110/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0643 Acc: 0.9893\n",
      "best acc is 0.9902\n",
      "Epoch 111/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0652 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 112/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0657 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 113/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0657 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 114/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0661 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 115/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0656 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 116/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0667 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 117/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0665 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 118/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0674 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 119/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0672 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 120/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0675 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 121/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0684 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 122/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0682 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 123/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0687 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 124/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0686 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 125/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0690 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 126/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0693 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 127/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0688 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 128/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0698 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 129/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0693 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 130/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0706 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 131/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0706 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 132/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0709 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 133/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0709 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 134/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0707 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 135/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0719 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 136/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0724 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 137/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0721 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 138/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0714 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 139/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0721 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 140/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0727 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 141/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0729 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 142/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0733 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 143/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0733 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 144/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0737 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 145/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0741 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 146/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0743 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 147/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0745 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 148/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0750 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 149/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0754 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 150/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0764 Acc: 0.9886\n",
      "best acc is 0.9902\n",
      "Epoch 151/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0762 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 152/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0764 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 153/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0760 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 154/210:\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0759 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 155/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0758 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 156/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0758 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 157/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0763 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 158/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0774 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 159/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0773 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 160/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0776 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 161/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0780 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 162/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0780 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 163/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0789 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 164/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0793 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 165/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0780 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 166/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0793 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 167/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0792 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 168/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0798 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 169/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0795 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 170/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0803 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 171/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0804 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 172/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0811 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 173/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0806 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 174/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0828 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 175/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0814 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 176/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0817 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 177/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0824 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 178/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0834 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 179/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0825 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 180/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0832 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 181/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0832 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 182/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0830 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 183/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0832 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 184/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0839 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 185/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0842 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 186/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0837 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 187/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0845 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 188/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0853 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 189/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0856 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 190/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0854 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 191/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0859 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 192/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0865 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 193/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0862 Acc: 0.9892\n",
      "best acc is 0.9902\n",
      "Epoch 194/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0871 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 195/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0863 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 196/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0877 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 197/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0878 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 198/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0870 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 199/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0883 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 200/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0892 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 201/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0880 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 202/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0890 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 203/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0904 Acc: 0.9885\n",
      "best acc is 0.9902\n",
      "Epoch 204/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0893 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 205/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0901 Acc: 0.9889\n",
      "best acc is 0.9902\n",
      "Epoch 206/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0905 Acc: 0.9888\n",
      "best acc is 0.9902\n",
      "Epoch 207/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0902 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Epoch 208/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0905 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 209/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0912 Acc: 0.9887\n",
      "best acc is 0.9902\n",
      "Epoch 210/210:\n",
      "----------\n",
      "Training Loss: 0.0000 Acc: 1.0000\n",
      "valid Loss: 0.0908 Acc: 0.9890\n",
      "best acc is 0.9902\n",
      "Training complete in 9m 41s\n",
      "Best val Acc: 0.990238\n"
     ]
    }
   ],
   "source": [
    "#Learning_rate=0.001\n",
    "loss_criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adam(model.parameters())\n",
    "epoch=210\n",
    "var_loss=np.inf\n",
    "train_losses,val_losses=[],[]\n",
    "import time,copy\n",
    "since = time.time()\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "model=model.to(device)\n",
    "for e in range(epoch):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    print('Epoch {}/{}:'.format(e+1, epoch))\n",
    "    print('-' * 10)\n",
    "    for images,labels in train_loader:\n",
    "        images,labels=images.to(device),labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        #images=images.view(images.shape[0],784)\n",
    "        output=model(images)\n",
    "        loss=loss_criterion(output,labels)\n",
    "        preds = torch.argmax(output,1)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "    epoch_loss = running_loss / len(xtrain)\n",
    "    train_losses.append(epoch_loss)\n",
    "    epoch_acc = running_corrects.double() / len(xtrain)\n",
    "    print('{} Loss: {:.4f} Acc: {:.4f}'.format(\"Training\", epoch_loss, epoch_acc))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            accuracy,epoch_loss=validation(model,val_loader,loss_criterion)\n",
    "            val_losses.append(epoch_loss)\n",
    "            if best_acc<accuracy:\n",
    "                best_acc = accuracy\n",
    "                torch.save(model.state_dict(),'model.pt')\n",
    "            else:\n",
    "                print(\"best acc is {:.4f}\".format(best_acc))\n",
    "    model.train()    \n",
    "time_elapsed = time.time() - since\n",
    "print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))\n",
    "print('Best val Acc: {:4f}'.format(best_acc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f3eba3a45d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses,label='Train Loss')\n",
    "plt.plot(val_losses,label='Validation Loss') \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pt'))\n",
    "test_=np.array(test).reshape(-1,1,28,28)/255\n",
    "testT=torch.tensor(test_)\n",
    "test_dataset=testT.type(torch.FloatTensor)\n",
    "test_loader=torch.utils.data.DataLoader(test_dataset,batch_size=val_batch_size,shuffle=False)\n",
    "model=model.to(device)\n",
    "with torch.no_grad():\n",
    "        model.eval()\n",
    "        label=[]\n",
    "        for images in test_loader:\n",
    "            images=images.to(device)\n",
    "            #images=images.reshape(images[0],784)\n",
    "            output=model(images)\n",
    "            labels=torch.argmax(output,1)\n",
    "            label+=labels\n",
    "        \n",
    "        sub={'ImageId':[i+1 for i in range(test.shape[0])],'Label':np.array(label)}\n",
    "        submission=pd.DataFrame(sub)\n",
    "        submission.to_csv('submissionpytorch1.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldlenv",
   "language": "python",
   "name": "mldlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
